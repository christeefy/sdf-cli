---
title: "SDF CLI Reference"
description: "This document contains the help content for the sdf command-line program."
---

# Command-Line Help for `sdf`

This document contains the help content for the `sdf` command-line program.

**Command Overview:**

* [`sdf`↴](#sdf)
* [`sdf new`↴](#sdf-new)
* [`sdf clean`↴](#sdf-clean)
* [`sdf build`↴](#sdf-build)
* [`sdf describe`↴](#sdf-describe)
* [`sdf deploy`↴](#sdf-deploy)
* [`sdf view`↴](#sdf-view)
* [`sdf lineage`↴](#sdf-lineage)
* [`sdf system`↴](#sdf-system)
* [`sdf auth`↴](#sdf-auth)
* [`sdf auth login`↴](#sdf-auth-login)
* [`sdf auth logout`↴](#sdf-auth-logout)
* [`sdf auth status`↴](#sdf-auth-status)
* [`sdf auth aws`↴](#sdf-auth-aws)
* [`sdf analyze`↴](#sdf-analyze)
* [`sdf analyze unused-columns`↴](#sdf-analyze-unused-columns)

## `sdf`

SDF's modular SQL

**Usage:** `sdf <COMMAND>`

###### **Subcommands:**

* `new` — Create a new SDF workspace at PATH
* `clean` — Remove generated SDF artifacts in workspace
* `build` — Build, run, and save SDF queries
* `describe` — Describe SDF table schemas
* `deploy` — Create a deployment and post it to the SDF service
* `view` — View existing table content
* `lineage` — Display lineage for a given column
* `system` — System maintenance and generation of reference documentation
* `auth` — Manage credentials and tokens
* `analyze` — Display results of various supported analyses (e.g. unused-columns)



## `sdf new`

Create a new SDF workspace at PATH

**Usage:** `sdf new [OPTIONS] [PATH]`

###### **Arguments:**

* `<PATH>` — Create a new SDF workspace at PATH

###### **Options:**

* `--quiet` — Do not print sdf console messages

  Default value: `false`
* `--log <LOG>` — Set log level (by setting env var RUST_LOG)
* `--perf <PERF>` — Performance Metric Log Level

  Default value: `0`
* `--list-samples` — List all available samples

  Default value: `false`
* `--sample <SAMPLE>` — Create a workspace with the sample content at path



## `sdf clean`

Remove generated SDF artifacts in workspace

**Usage:** `sdf clean [OPTIONS] [INPUT_FILES]...`

###### **Arguments:**

* `<INPUT_FILES>` — Remove all derived artifacts from a workspace, where workspace is identified by any input location or, if none is given, by the current working directory

###### **Options:**

* `--quiet` — Do not print sdf console messages

  Default value: `false`
* `--log <LOG>` — Set log level (by setting env var RUST_LOG)
* `--perf <PERF>` — Performance Metric Log Level

  Default value: `0`
* `--cache-directory <CACHE_DIRECTORY>` — Sets the 'cache' directory which will contain all output files. Normally, this is `.sdfcache` under the workspace root
* `-p`, `--profile <PROFILE>` — Clean cache entries for this profile



## `sdf build`

Build, run, and save SDF queries

**Usage:** `sdf build [OPTIONS] [INPUT_FILES]...`

###### **Arguments:**

* `<INPUT_FILES>` — Build these *.sql files or [[catalog.]schema.]table targets; providing no input or optionally a */workspace.sdf.yml file rebuilds the whole workspace

###### **Options:**

* `--quiet` — Do not print sdf console messages

  Default value: `false`
* `--log <LOG>` — Set log level (by setting env var RUST_LOG)
* `--perf <PERF>` — Performance Metric Log Level

  Default value: `0`
* `--cache-directory <CACHE_DIRECTORY>` — Sets the 'cache' directory which will contain all output files. Normally, this is `.sdfcache` under the workspace root
* `--var <VAR>` — Var bindings of the form: n=1 or year="2022 01 01", only string variables supported
* `--date <DATE>` — Run all tasks that run on this datetime, [ENV] use prefixes of RFC format, e.g. 2020-12-31T21:07, or 2020-12-31, etc
* `--from <FROM>` — Backfill from this datetime (inclusive), use prefixes of RFC format, e.g. 2020-12-31T21:07, or 2020-12-31, etc
* `--to <TO>` — Backfill to this datetime (exclusive), use prefixes of RFC format, e.g. 2020-12-31T21:07, or 2020-12-31, etc (default is now)
* `--upload` — Uploads tables to workspace's remote-location (if need to), a user specified s3://bucket

  Default value: `false`
* `--download` — Downloads tables to the workspace's cache (if need to), a user specified s3://bucket

  Default value: `false`
* `-p`, `--profile <PROFILE>` — Build data using this profile
* `-t`, `--test-profile <TEST_PROFILE>` — Build data using the given --profile but renamed to this
* `--dry-run` — Plan but don't execute backfills, output as a shell script in .sdfcache/{profile}/commands.sh

  Default value: `false`
* `--file-format <FILE_FORMAT>` — Save tables in this format

  Default value: `parquet`

  Possible values: `parquet`, `csv`, `json`

* `--print-format <PRINT_FORMAT>` — Show tables in this format

  Default value: `table`

  Possible values: `table`, `csv`, `tsv`, `json`, `nd-json`

* `--no-save` — Don't save non-source tables

  Default value: `false`
* `--no-cache` — Don't cache tables

  Default value: `false`
* `--no-workflow` — Don't run a workflow, just the query

  Default value: `false`
* `--limit <LIMIT>` — "Limiting number of shown rows. Run with --limit 0 to remove limit.",

  Default value: `10`
* `--no-show` — Don't show the result of queries

  Default value: `false`
* `--no-assembly` — Do not generate a assembly.sdf.yml file

  Default value: `false`
* `--output-file-contents-on-error` — Include file contents when generating error output

  Default value: `false`



## `sdf describe`

Describe SDF table schemas

**Usage:** `sdf describe [OPTIONS] [INPUT_FILES]...`

###### **Arguments:**

* `<INPUT_FILES>` — Analyze these *.sql files or [[catalog.]schema.]table targets; providing no input or optionally a */workspace.sdf.yml file rebuilds the whole workspace

###### **Options:**

* `--quiet` — Do not print sdf console messages

  Default value: `false`
* `--log <LOG>` — Set log level (by setting env var RUST_LOG)
* `--perf <PERF>` — Performance Metric Log Level

  Default value: `0`
* `--cache-directory <CACHE_DIRECTORY>` — Sets the 'cache' directory which will contain all output files. Normally, this is `.sdfcache` under the workspace root
* `--no-propagate` — Propagate labels

  Default value: `false`
* `--region <REGION>` — The AWS region (default=us-east-1))
* `-p`, `--profile <PROFILE>` — Describe data using this profile
* `-t`, `--test-profile <TEST_PROFILE>` — Build data using the given --profile but renamed to this
* `--no-save` — Don't save schema tables

  Default value: `false`
* `--print-format <PRINT_FORMAT>` — Show tables in this format

  Default value: `table`

  Possible values: `table`, `yml`, `json`

* `--file-format <FILE_FORMAT>` — Save tables in this format

  Default value: `json`

  Possible values: `table`, `yml`, `json`

* `--save-filter <SAVE_FILTER>` — Save only what is being filtered

  Default value: `assembly`

  Possible values:
  - `targets-only`:
    Saves the target.sdf.yml files in the .sdfcache
  - `assembly`:
    Saves all meta build info as an assembly

* `--no-show` — Don't show the result of queries

  Default value: `false`
* `--no-summary` — Do not generate a all.sdf.yml file

  Default value: `false`
* `--output-file-contents-on-error` — Include file contents when generating error output

  Default value: `false`
* `--generate-information-schema` — Generate information schemas and metadata

  Default value: `false`
* `--code-reports` — Run code reports against information schemas and metadata

  Default value: `false`
* `--no-code-checks` — Run code checks against information schemas and metadata

  Default value: `false`
* `--check-access` — Check attribute based access control, for testing assumes a user.json and a job.json file in workspace directory



## `sdf deploy`

Create a deployment and post it to the SDF service

**Usage:** `sdf deploy [OPTIONS] [INPUT_FILES]...`

###### **Arguments:**

* `<INPUT_FILES>` — Deploy these *.sql files or [[catalog.]schema.]table targets; providing no input or optionally a */workspace.sdf.yml file rebuilds the whole workspace

###### **Options:**

* `--quiet` — Do not print sdf console messages

  Default value: `false`
* `--log <LOG>` — Set log level (by setting env var RUST_LOG)
* `--perf <PERF>` — Performance Metric Log Level

  Default value: `0`
* `--cache-directory <CACHE_DIRECTORY>` — Sets the 'cache' directory which will contain all output files. Normally, this is `.sdfcache` under the workspace root
* `--date <DATE>` — Backfill all tasks that run (only) on this datetime, [ENV] use prefixes of RFC format, e.g. 2020-12-31T21:07, or 2020-12-31, etc
* `--from <FROM>` — Backfill from this datetime (inclusive), use prefixes of RFC format, e.g. 2020-12-31T21:07 or 2020-12-31
* `--to <TO>` — Backfill to this datetime (exclusive), use RFC format, e.g., e.g. 2020-12-31T21:07:14-07:00, with optional T part
* `-p`, `--profile <PROFILE>` — Deploy build artifacts using this profile
* `--execute` — When to execute this job run; defaults to recurring: if --from/--to are given

  Default value: `false`
* `--dry-run` — Create a deploy.json and archive.tar.gz but don't post it to the service

  Default value: `false`
* `--no-propagate` — Propagate labels for deployed targets

  Default value: `false`



## `sdf view`

View existing table content

**Usage:** `sdf view [OPTIONS] <INPUT_FILE>`

###### **Arguments:**

* `<INPUT_FILE>` — Input is catalog.schema.table target, or a .json, .csv, or .parquet file, or a directory ending in '/' containing parquet files

###### **Options:**

* `--quiet` — Do not print sdf console messages

  Default value: `false`
* `--log <LOG>` — Set log level (by setting env var RUST_LOG)
* `--perf <PERF>` — Performance Metric Log Level

  Default value: `0`
* `--cache-directory <CACHE_DIRECTORY>` — Sets the 'cache' directory which will contain all output files. Normally, this is `.sdfcache` under the workspace root
* `--date <DATE>` — View catalog.schema.table of this datetime, [ENV] use prefixes of RFC format, e.g. 2020-12-31T21:07, or 2020-12-31 with default T00:00, etc
* `--from <FROM>` — View catalog.schema.table from this datetime (inclusive), use prefixes of RFC format, e.g. 2020-12-31T21:07, or 2020-12-31 with default T00:00
* `--to <TO>` — View catalog.schema.table to this datetime (exclusive), use prefixes of RFC format, e.g. 2020-12-31T21:07, or 2020-12-31 with default T23:59
* `-p`, `--profile <PROFILE>` — View data using this profile
* `--limit <LIMIT>` — "Limiting number of shown rows. Run with --limit 0 to remove limit.",

  Default value: `10`
* `--print-format <PRINT_FORMAT>` — Show tables in this format

  Default value: `table`

  Possible values: `table`, `csv`, `tsv`, `json`, `nd-json`

* `--file-format <FILE_FORMAT>` — Read tables having this format

  Default value: `parquet`

  Possible values: `parquet`, `csv`, `json`

* `--no-header` — Assumes that .csv files have a header, use --no-header if they have none

  Default value: `false`



## `sdf lineage`

Display lineage for a given column

**Usage:** `sdf lineage [OPTIONS] [INPUT_FILES]...`

###### **Arguments:**

* `<INPUT_FILES>` — The specification of the table for whose columns to compute lineage

###### **Options:**

* `--quiet` — Do not print sdf console messages

  Default value: `false`
* `--log <LOG>` — Set log level (by setting env var RUST_LOG)
* `--perf <PERF>` — Performance Metric Log Level

  Default value: `0`
* `--cache-directory <CACHE_DIRECTORY>` — Sets the 'cache' directory which will contain all output files. Normally, this is `.sdfcache` under the workspace root
* `-p`, `--profile <PROFILE>` — Build data using this profile
* `--column <COLUMN>` — The column for which to compute lineage
* `--forward` — Compute downstream lineage instead of the default upstream lineage

  Default value: `false`
* `-t`, `--test-profile <TEST_PROFILE>` — Build data using the given --profile but renamed to this
* `--show-scans` — Display scan dependencies in addition to copy and mod dependencies. Unset by default

  Default value: `false`



## `sdf system`

System maintenance and generation of reference documentation

**Usage:** `sdf system [OPTIONS]`

###### **Options:**

* `--quiet` — Do not print sdf console messages

  Default value: `false`
* `--log <LOG>` — Set log level (by setting env var RUST_LOG)
* `--perf <PERF>` — Performance Metric Log Level

  Default value: `0`
* `--show <SHOW>` — Print reference documents on stdout

  Possible values:
  - `cli-markdown`:
    Print cli commands and options in markdown format
  - `sdf-json-schema`:
    Print json schema for *.sdf.yml documents
  - `deploy-json-schema`:
    Print json schema for deploy.json documents
  - `functions`:
    Print all supported supported functions of given dialect in given format

* `--update` — Update SDF in place to the latest version

  Default value: `false`
* `--uninstall` — Uninstall SDF from the system

  Default value: `false`
* `--dialect <DIALECT>` — Pick a dialect to print

  Default value: `sdf`

  Possible values:
  - `sdf`:
    SDF Dialect (which is ansi++)
  - `spark`:
    Spark Dialect
  - `presto`:
    Presto dialect
  - `postgresql`:
    Postgres dialect
  - `bigquery`:
    BigQuery dialect
  - `legacy-bigquery`:
    LegacyBigQuery dialect
  - `redshift`

* `--format <FORMAT>` — Pick a format to print in

  Default value: `markdown`

  Possible values: `markdown`, `yaml`

* `--section <SECTION>` — Pick a section to print

  Default value: ``



## `sdf auth`

Manage credentials and tokens

**Usage:** `sdf auth <COMMAND>`

###### **Subcommands:**

* `login` — Obtain credentials for accessing remote resources
* `logout` — Log out of SDF Service
* `status` — Show status of credentials / tokens
* `aws` — Configure how to authenticate with AWS



## `sdf auth login`

Obtain credentials for accessing remote resources

**Usage:** `sdf auth login [OPTIONS]`

###### **Options:**

* `--id-token <ID_TOKEN>` — Path to a file containing an OIDC identity token (a JWT)



## `sdf auth logout`

Log out of SDF Service

**Usage:** `sdf auth logout`



## `sdf auth status`

Show status of credentials / tokens

**Usage:** `sdf auth status`



## `sdf auth aws`

Configure how to authenticate with AWS

**Usage:** `sdf auth aws [OPTIONS]`

###### **Options:**

* `--default-region <DEFAULT_REGION>` — AWS Region to use

  Default value: `us-east-1`
* `--profile <PROFILE>` — AWS profile to use, as usually defined in ~/.aws/config
* `--role-arn <ROLE_ARN>` — ARN of the role to assume
* `--external-id <EXTERNAL_ID>` — ARN of the role to assume
* `--use-web-identity` — ARN of the role to assume

  Default value: `false`
* `--bucket-region-map <BUCKET_REGION_MAP>` — Mapping of bucket names to regions



## `sdf analyze`

Display results of various supported analyses (e.g. unused-columns)

**Usage:** `sdf analyze [OPTIONS] <COMMAND>`

###### **Subcommands:**

* `unused-columns` — Analyze upstream tables for unused columns

###### **Options:**

* `--quiet` — Do not print sdf console messages

  Default value: `false`
* `--log <LOG>` — Set log level (by setting env var RUST_LOG)
* `--perf <PERF>` — Performance Metric Log Level

  Default value: `0`
* `--cache-directory <CACHE_DIRECTORY>` — Sets the 'cache' directory which will contain all output files. Normally, this is `.sdfcache` under the workspace root



## `sdf analyze unused-columns`

Analyze upstream tables for unused columns

**Usage:** `sdf analyze unused-columns [OPTIONS] [INPUT_FILES]...`

###### **Arguments:**

* `<INPUT_FILES>` — The specification of the output tables used as roots for computing unused upstream columns

###### **Options:**

* `--quiet` — Do not print sdf console messages

  Default value: `false`
* `--log <LOG>` — Set log level (by setting env var RUST_LOG)
* `--perf <PERF>` — Performance Metric Log Level

  Default value: `0`
* `-p`, `--profile <PROFILE>` — Analyze queries using this profile
* `-t`, `--test-profile <TEST_PROFILE>` — Build data using the given --profile but renamed to this



   Finished system in 0.008 secs

