---
title: "SDF CLI Reference"
description: "This document contains the help content for the sdf command-line program."
---

# Command-Line Help for `sdf`

This document contains the help content for the `sdf` command-line program.

**Command Overview:**

* [`sdf`↴](#sdf)
* [`sdf new`↴](#sdf-new)
* [`sdf clean`↴](#sdf-clean)
* [`sdf build`↴](#sdf-build)
* [`sdf describe`↴](#sdf-describe)
* [`sdf deploy`↴](#sdf-deploy)
* [`sdf view`↴](#sdf-view)
* [`sdf lineage`↴](#sdf-lineage)
* [`sdf system`↴](#sdf-system)
* [`sdf auth`↴](#sdf-auth)
* [`sdf auth login`↴](#sdf-auth-login)
* [`sdf auth logout`↴](#sdf-auth-logout)
* [`sdf auth status`↴](#sdf-auth-status)
* [`sdf auth aws`↴](#sdf-auth-aws)
* [`sdf analyze`↴](#sdf-analyze)
* [`sdf analyze unused-columns`↴](#sdf-analyze-unused-columns)
* [`sdf show`↴](#sdf-show)
* [`sdf test`↴](#sdf-test)
* [`sdf query`↴](#sdf-query)
* [`sdf obfuscate`↴](#sdf-obfuscate)

## `sdf`

SDF's modular SQL

**Usage:** `sdf <COMMAND>`

###### **Subcommands:**

* `new` — Create a new SDF workspace at PATH
* `clean` — Remove generated SDF artifacts in workspace
* `build` — Build, run, and save SDF queries
* `describe` — Describe inferred table schemas including propagated classifiers, run code code checks
* `deploy` — Create a deployment and post it to the SDF service
* `view` — View existing table content
* `lineage` — Display lineage for a given column
* `system` — System maintenance, install and update
* `auth` — Manage credentials and tokens
* `analyze` — Display results of various supported analyses (e.g. unused-columns)
* `show` — Generation of reference documentation: for CLI, metadata definition blocks, information schema and functions
* `test` — Generation of reference documentation: for CLI, metadata definition blocks, information schema and functions
* `query` — Generation of reference documentation: for CLI, metadata definition blocks, information schema and functions
* `obfuscate` — Obfuscate the workspace into the provided directory



## `sdf new`

Create a new SDF workspace at PATH

**Usage:** `sdf new [OPTIONS] [PATH]`

###### **Arguments:**

* `<PATH>` — Create a new SDF workspace at PATH

###### **Options:**

* `--list-samples` — List all available samples

  Default value: `false`
* `--sample <SAMPLE>` — Create a workspace with the sample content at path
* `--quiet` — Do not print sdf console messages

  Default value: `false`
* `--log <LOG>` — Set log level (by setting env var RUST_LOG)
* `--perf <PERF>` — Performance Metric Log Level

  Default value: `0`
* `--dedup` — Deduplicate compiler error messages. This has a slight memory overhead

  Default value: `false`
* `--summary <SUMMARY>` — Save summary of analyzed source files to this file
* `--num-threads <NUM_THREADS>` — Number of threads to use for parallel operations. If unset, default is number of CPU cores. Must be greater than 0 if set
* `--stack-size <STACK_SIZE>` — Set stack size for worker thread in bytes
* `--zero-stmt-sources <ZERO_STMT_SOURCES>` — Set the skip list file
* `--runlog <RUNLOG>` — Save the runlog to this file



## `sdf clean`

Remove generated SDF artifacts in workspace

**Usage:** `sdf clean [OPTIONS] [INPUT_FILES]...`

###### **Arguments:**

* `<INPUT_FILES>` — Remove all derived artifacts from a workspace, where workspace is identified by any input location or, if none is given, by the current working directory

###### **Options:**

* `--profile <PROFILE>` — Clean cache entries for this profile
* `--quiet` — Do not print sdf console messages

  Default value: `false`
* `--log <LOG>` — Set log level (by setting env var RUST_LOG)
* `--perf <PERF>` — Performance Metric Log Level

  Default value: `0`
* `--dedup` — Deduplicate compiler error messages. This has a slight memory overhead

  Default value: `false`
* `--summary <SUMMARY>` — Save summary of analyzed source files to this file
* `--num-threads <NUM_THREADS>` — Number of threads to use for parallel operations. If unset, default is number of CPU cores. Must be greater than 0 if set
* `--stack-size <STACK_SIZE>` — Set stack size for worker thread in bytes
* `--zero-stmt-sources <ZERO_STMT_SOURCES>` — Set the skip list file
* `--runlog <RUNLOG>` — Save the runlog to this file
* `--cache-dir <CACHE_DIR>` — Sets the 'cache' directory which will contain all output files. Normally, this is `.sdfcache` under the workspace root
* `--credentials-dir <CREDENTIALS_DIR>`



## `sdf build`

Build, run, and save SDF queries

**Usage:** `sdf build [OPTIONS] [INPUT_FILES]...`

###### **Arguments:**

* `<INPUT_FILES>` — Build these *.sql files or [[catalog.]schema.]table targets; providing no input or a path to workspace.sdf.yml rebuilds the whole workspace

###### **Options:**

* `--profile <PROFILE>` — Build data using this profile
* `--date <DATE>` — Run all tasks that run on this datetime, [ENV] use prefixes of RFC format, e.g. 2020-12-31T21:07, or 2020-12-31, etc
* `--from <FROM>` — Backfill from this datetime (inclusive), use prefixes of RFC format, e.g. 2020-12-31T21:07, or 2020-12-31, etc
* `--to <TO>` — Backfill to this datetime (exclusive), use prefixes of RFC format, e.g. 2020-12-31T21:07, or 2020-12-31, etc (default is now)
* `--dry-run` — Plan but don't execute backfills, output as a shell script in .sdfcache/{profile}/commands.sh

  Default value: `false`
* `--var <VAR>` — Provide var bindings of the form: n=1 or year="2022 01 01", only string variables supported
* `--file-data-format <FILE_DATA_FORMAT>` — Save tables in this format

  Default value: `parquet`

  Possible values: `parquet`, `csv`, `json`

* `--print-data-format <PRINT_DATA_FORMAT>` — Show tables in this format

  Default value: `table`

  Possible values: `table`, `csv`, `tsv`, `json`, `nd-json`

* `--no-cache` — Don't read and write cached table data

  Default value: `false`
* `--cache-use <CACHE_USE>` — Controls read and write cached table data

  Default value: `read-write`

  Possible values: `none`, `read-only`, `read-write`, `write-only`

* `--no-workflow` — Don't run a workflow, just the query

  Default value: `false`
* `--limit <LIMIT>` — "Limiting number of shown rows. Run with --limit 0 to remove limit.",

  Default value: `10`
* `--job-time-limit-s <JOB_TIME_LIMIT_S>` — Impose a hard time limit on the execution of each query. Default is 30 seconds

  Default value: `30`
* `--no-show` — Don't show the result of queries

  Default value: `false`
* `--print-meta-format <PRINT_META_FORMAT>` — Show tables in this format

  Default value: `table`

  Possible values: `table`, `yml`, `json`

* `--generate-information-schema` — Generate information schemas and metadata

  Default value: `false`
* `--no-propagate` — Propagate labels

  Default value: `false`
* `--output-file-contents-on-error` — Include file contents when generating error output

  Default value: `false`
* `--save-assembly` — Save assembly in yaml format

  Default value: `false`
* `--no-parallel` — Do NOT run in parallel
* `--infer-functions` — When queries make use of undeclared functions, generate implied signatures based on how the function is called instead of failing

  Default value: `false`
* `--upload` — Uploads tables to workspace's remote-location (if need to), a user specified s3://bucket

  Default value: `false`
* `--download` — Downloads tables to the workspace's cache (if need to), a user specified s3://bucket

  Default value: `false`
* `--quiet` — Do not print sdf console messages

  Default value: `false`
* `--log <LOG>` — Set log level (by setting env var RUST_LOG)
* `--perf <PERF>` — Performance Metric Log Level

  Default value: `0`
* `--dedup` — Deduplicate compiler error messages. This has a slight memory overhead

  Default value: `false`
* `--summary <SUMMARY>` — Save summary of analyzed source files to this file
* `--num-threads <NUM_THREADS>` — Number of threads to use for parallel operations. If unset, default is number of CPU cores. Must be greater than 0 if set
* `--stack-size <STACK_SIZE>` — Set stack size for worker thread in bytes
* `--zero-stmt-sources <ZERO_STMT_SOURCES>` — Set the skip list file
* `--runlog <RUNLOG>` — Save the runlog to this file
* `--cache-dir <CACHE_DIR>` — Sets the 'cache' directory which will contain all output files. Normally, this is `.sdfcache` under the workspace root
* `--credentials-dir <CREDENTIALS_DIR>`



## `sdf describe`

Describe inferred table schemas including propagated classifiers, run code code checks

**Usage:** `sdf describe [OPTIONS] [INPUT_FILES]...`

###### **Arguments:**

* `<INPUT_FILES>` — Build these *.sql files or [[catalog.]schema.]table targets; providing no input or a path to workspace.sdf.yml rebuilds the whole workspace

###### **Options:**

* `--profile <PROFILE>` — Build data using this profile
* `--date <DATE>` — Run all tasks that run on this datetime, [ENV] use prefixes of RFC format, e.g. 2020-12-31T21:07, or 2020-12-31, etc
* `--from <FROM>` — Backfill from this datetime (inclusive), use prefixes of RFC format, e.g. 2020-12-31T21:07, or 2020-12-31, etc
* `--to <TO>` — Backfill to this datetime (exclusive), use prefixes of RFC format, e.g. 2020-12-31T21:07, or 2020-12-31, etc (default is now)
* `--dry-run` — Plan but don't execute backfills, output as a shell script in .sdfcache/{profile}/commands.sh

  Default value: `false`
* `--var <VAR>` — Provide var bindings of the form: n=1 or year="2022 01 01", only string variables supported
* `--file-data-format <FILE_DATA_FORMAT>` — Save tables in this format

  Default value: `parquet`

  Possible values: `parquet`, `csv`, `json`

* `--print-data-format <PRINT_DATA_FORMAT>` — Show tables in this format

  Default value: `table`

  Possible values: `table`, `csv`, `tsv`, `json`, `nd-json`

* `--no-cache` — Don't read and write cached table data

  Default value: `false`
* `--cache-use <CACHE_USE>` — Controls read and write cached table data

  Default value: `read-write`

  Possible values: `none`, `read-only`, `read-write`, `write-only`

* `--no-workflow` — Don't run a workflow, just the query

  Default value: `false`
* `--limit <LIMIT>` — "Limiting number of shown rows. Run with --limit 0 to remove limit.",

  Default value: `10`
* `--job-time-limit-s <JOB_TIME_LIMIT_S>` — Impose a hard time limit on the execution of each query. Default is 30 seconds

  Default value: `30`
* `--no-show` — Don't show the result of queries

  Default value: `false`
* `--print-meta-format <PRINT_META_FORMAT>` — Show tables in this format

  Default value: `table`

  Possible values: `table`, `yml`, `json`

* `--generate-information-schema` — Generate information schemas and metadata

  Default value: `false`
* `--no-propagate` — Propagate labels

  Default value: `false`
* `--output-file-contents-on-error` — Include file contents when generating error output

  Default value: `false`
* `--save-assembly` — Save assembly in yaml format

  Default value: `false`
* `--no-parallel` — Do NOT run in parallel
* `--infer-functions` — When queries make use of undeclared functions, generate implied signatures based on how the function is called instead of failing

  Default value: `false`
* `--upload` — Uploads tables to workspace's remote-location (if need to), a user specified s3://bucket

  Default value: `false`
* `--download` — Downloads tables to the workspace's cache (if need to), a user specified s3://bucket

  Default value: `false`
* `--quiet` — Do not print sdf console messages

  Default value: `false`
* `--log <LOG>` — Set log level (by setting env var RUST_LOG)
* `--perf <PERF>` — Performance Metric Log Level

  Default value: `0`
* `--dedup` — Deduplicate compiler error messages. This has a slight memory overhead

  Default value: `false`
* `--summary <SUMMARY>` — Save summary of analyzed source files to this file
* `--num-threads <NUM_THREADS>` — Number of threads to use for parallel operations. If unset, default is number of CPU cores. Must be greater than 0 if set
* `--stack-size <STACK_SIZE>` — Set stack size for worker thread in bytes
* `--zero-stmt-sources <ZERO_STMT_SOURCES>` — Set the skip list file
* `--runlog <RUNLOG>` — Save the runlog to this file
* `--cache-dir <CACHE_DIR>` — Sets the 'cache' directory which will contain all output files. Normally, this is `.sdfcache` under the workspace root
* `--credentials-dir <CREDENTIALS_DIR>`



## `sdf deploy`

Create a deployment and post it to the SDF service

**Usage:** `sdf deploy [OPTIONS] [INPUT_FILES]...`

###### **Arguments:**

* `<INPUT_FILES>` — Build these *.sql files or [[catalog.]schema.]table targets; providing no input or a path to workspace.sdf.yml rebuilds the whole workspace

###### **Options:**

* `--profile <PROFILE>` — Build data using this profile
* `--date <DATE>` — Run all tasks that run on this datetime, [ENV] use prefixes of RFC format, e.g. 2020-12-31T21:07, or 2020-12-31, etc
* `--from <FROM>` — Backfill from this datetime (inclusive), use prefixes of RFC format, e.g. 2020-12-31T21:07, or 2020-12-31, etc
* `--to <TO>` — Backfill to this datetime (exclusive), use prefixes of RFC format, e.g. 2020-12-31T21:07, or 2020-12-31, etc (default is now)
* `--dry-run` — Plan but don't execute backfills, output as a shell script in .sdfcache/{profile}/commands.sh

  Default value: `false`
* `--var <VAR>` — Provide var bindings of the form: n=1 or year="2022 01 01", only string variables supported
* `--file-data-format <FILE_DATA_FORMAT>` — Save tables in this format

  Default value: `parquet`

  Possible values: `parquet`, `csv`, `json`

* `--print-data-format <PRINT_DATA_FORMAT>` — Show tables in this format

  Default value: `table`

  Possible values: `table`, `csv`, `tsv`, `json`, `nd-json`

* `--no-cache` — Don't read and write cached table data

  Default value: `false`
* `--cache-use <CACHE_USE>` — Controls read and write cached table data

  Default value: `read-write`

  Possible values: `none`, `read-only`, `read-write`, `write-only`

* `--no-workflow` — Don't run a workflow, just the query

  Default value: `false`
* `--limit <LIMIT>` — "Limiting number of shown rows. Run with --limit 0 to remove limit.",

  Default value: `10`
* `--job-time-limit-s <JOB_TIME_LIMIT_S>` — Impose a hard time limit on the execution of each query. Default is 30 seconds

  Default value: `30`
* `--no-show` — Don't show the result of queries

  Default value: `false`
* `--print-meta-format <PRINT_META_FORMAT>` — Show tables in this format

  Default value: `table`

  Possible values: `table`, `yml`, `json`

* `--generate-information-schema` — Generate information schemas and metadata

  Default value: `false`
* `--no-propagate` — Propagate labels

  Default value: `false`
* `--output-file-contents-on-error` — Include file contents when generating error output

  Default value: `false`
* `--save-assembly` — Save assembly in yaml format

  Default value: `false`
* `--no-parallel` — Do NOT run in parallel
* `--infer-functions` — When queries make use of undeclared functions, generate implied signatures based on how the function is called instead of failing

  Default value: `false`
* `--upload` — Uploads tables to workspace's remote-location (if need to), a user specified s3://bucket

  Default value: `false`
* `--download` — Downloads tables to the workspace's cache (if need to), a user specified s3://bucket

  Default value: `false`
* `--quiet` — Do not print sdf console messages

  Default value: `false`
* `--log <LOG>` — Set log level (by setting env var RUST_LOG)
* `--perf <PERF>` — Performance Metric Log Level

  Default value: `0`
* `--dedup` — Deduplicate compiler error messages. This has a slight memory overhead

  Default value: `false`
* `--summary <SUMMARY>` — Save summary of analyzed source files to this file
* `--num-threads <NUM_THREADS>` — Number of threads to use for parallel operations. If unset, default is number of CPU cores. Must be greater than 0 if set
* `--stack-size <STACK_SIZE>` — Set stack size for worker thread in bytes
* `--zero-stmt-sources <ZERO_STMT_SOURCES>` — Set the skip list file
* `--runlog <RUNLOG>` — Save the runlog to this file
* `--cache-dir <CACHE_DIR>` — Sets the 'cache' directory which will contain all output files. Normally, this is `.sdfcache` under the workspace root
* `--credentials-dir <CREDENTIALS_DIR>`
* `--execute` — When to execute this job run; defaults to recurring: if --from/--to are given

  Default value: `false`
* `--delete` — Delete this workspace from the SDF Console

  Default value: `false`
* `-y`, `--yes` — Answer yes to all prompts

  Default value: `false`



## `sdf view`

View existing table content

**Usage:** `sdf view [OPTIONS] <INPUT_FILE>`

###### **Arguments:**

* `<INPUT_FILE>` — Input is catalog.schema.table target, or a .json, .csv, or .parquet file, or a directory ending in '/' containing parquet files

###### **Options:**

* `--date <DATE>` — View catalog.schema.table of this datetime, [ENV] use prefixes of RFC format, e.g. 2020-12-31T21:07, or 2020-12-31 with default T00:00, etc
* `--from <FROM>` — View catalog.schema.table from this datetime (inclusive), use prefixes of RFC format, e.g. 2020-12-31T21:07, or 2020-12-31 with default T00:00
* `--to <TO>` — View catalog.schema.table to this datetime (exclusive), use prefixes of RFC format, e.g. 2020-12-31T21:07, or 2020-12-31 with default T23:59
* `--profile <PROFILE>` — View data using this profile
* `--limit <LIMIT>` — "Limiting number of shown rows. Run with --limit 0 to remove limit.",

  Default value: `10`
* `--print-format <PRINT_FORMAT>` — Show tables in this format

  Default value: `table`

  Possible values: `table`, `csv`, `tsv`, `json`, `nd-json`

* `--file-format <FILE_FORMAT>` — Read tables having this format

  Default value: `parquet`

  Possible values: `parquet`, `csv`, `json`

* `--no-header` — Assumes that .csv files have a header, use --no-header if they have none

  Default value: `false`
* `--quiet` — Do not print sdf console messages

  Default value: `false`
* `--log <LOG>` — Set log level (by setting env var RUST_LOG)
* `--perf <PERF>` — Performance Metric Log Level

  Default value: `0`
* `--dedup` — Deduplicate compiler error messages. This has a slight memory overhead

  Default value: `false`
* `--summary <SUMMARY>` — Save summary of analyzed source files to this file
* `--num-threads <NUM_THREADS>` — Number of threads to use for parallel operations. If unset, default is number of CPU cores. Must be greater than 0 if set
* `--stack-size <STACK_SIZE>` — Set stack size for worker thread in bytes
* `--zero-stmt-sources <ZERO_STMT_SOURCES>` — Set the skip list file
* `--runlog <RUNLOG>` — Save the runlog to this file
* `--cache-dir <CACHE_DIR>` — Sets the 'cache' directory which will contain all output files. Normally, this is `.sdfcache` under the workspace root
* `--credentials-dir <CREDENTIALS_DIR>`



## `sdf lineage`

Display lineage for a given column

**Usage:** `sdf lineage [OPTIONS] [INPUT_FILES]...`

###### **Arguments:**

* `<INPUT_FILES>` — Build these *.sql files or [[catalog.]schema.]table targets; providing no input or a path to workspace.sdf.yml rebuilds the whole workspace

###### **Options:**

* `--profile <PROFILE>` — Build data using this profile
* `--date <DATE>` — Run all tasks that run on this datetime, [ENV] use prefixes of RFC format, e.g. 2020-12-31T21:07, or 2020-12-31, etc
* `--from <FROM>` — Backfill from this datetime (inclusive), use prefixes of RFC format, e.g. 2020-12-31T21:07, or 2020-12-31, etc
* `--to <TO>` — Backfill to this datetime (exclusive), use prefixes of RFC format, e.g. 2020-12-31T21:07, or 2020-12-31, etc (default is now)
* `--dry-run` — Plan but don't execute backfills, output as a shell script in .sdfcache/{profile}/commands.sh

  Default value: `false`
* `--var <VAR>` — Provide var bindings of the form: n=1 or year="2022 01 01", only string variables supported
* `--file-data-format <FILE_DATA_FORMAT>` — Save tables in this format

  Default value: `parquet`

  Possible values: `parquet`, `csv`, `json`

* `--print-data-format <PRINT_DATA_FORMAT>` — Show tables in this format

  Default value: `table`

  Possible values: `table`, `csv`, `tsv`, `json`, `nd-json`

* `--no-cache` — Don't read and write cached table data

  Default value: `false`
* `--cache-use <CACHE_USE>` — Controls read and write cached table data

  Default value: `read-write`

  Possible values: `none`, `read-only`, `read-write`, `write-only`

* `--no-workflow` — Don't run a workflow, just the query

  Default value: `false`
* `--limit <LIMIT>` — "Limiting number of shown rows. Run with --limit 0 to remove limit.",

  Default value: `10`
* `--job-time-limit-s <JOB_TIME_LIMIT_S>` — Impose a hard time limit on the execution of each query. Default is 30 seconds

  Default value: `30`
* `--no-show` — Don't show the result of queries

  Default value: `false`
* `--print-meta-format <PRINT_META_FORMAT>` — Show tables in this format

  Default value: `table`

  Possible values: `table`, `yml`, `json`

* `--generate-information-schema` — Generate information schemas and metadata

  Default value: `false`
* `--no-propagate` — Propagate labels

  Default value: `false`
* `--output-file-contents-on-error` — Include file contents when generating error output

  Default value: `false`
* `--save-assembly` — Save assembly in yaml format

  Default value: `false`
* `--no-parallel` — Do NOT run in parallel
* `--infer-functions` — When queries make use of undeclared functions, generate implied signatures based on how the function is called instead of failing

  Default value: `false`
* `--upload` — Uploads tables to workspace's remote-location (if need to), a user specified s3://bucket

  Default value: `false`
* `--download` — Downloads tables to the workspace's cache (if need to), a user specified s3://bucket

  Default value: `false`
* `--quiet` — Do not print sdf console messages

  Default value: `false`
* `--log <LOG>` — Set log level (by setting env var RUST_LOG)
* `--perf <PERF>` — Performance Metric Log Level

  Default value: `0`
* `--dedup` — Deduplicate compiler error messages. This has a slight memory overhead

  Default value: `false`
* `--summary <SUMMARY>` — Save summary of analyzed source files to this file
* `--num-threads <NUM_THREADS>` — Number of threads to use for parallel operations. If unset, default is number of CPU cores. Must be greater than 0 if set
* `--stack-size <STACK_SIZE>` — Set stack size for worker thread in bytes
* `--zero-stmt-sources <ZERO_STMT_SOURCES>` — Set the skip list file
* `--runlog <RUNLOG>` — Save the runlog to this file
* `--cache-dir <CACHE_DIR>` — Sets the 'cache' directory which will contain all output files. Normally, this is `.sdfcache` under the workspace root
* `--credentials-dir <CREDENTIALS_DIR>`
* `--column <COLUMN>` — The column for which to compute lineage
* `--forward` — Compute downstream lineage instead of the default upstream lineage

  Default value: `false`
* `--show-scans` — Display scan dependencies in addition to copy and mod dependencies. Unset by default

  Default value: `false`



## `sdf system`

System maintenance, install and update

**Usage:** `sdf system [OPTIONS]`

###### **Options:**

* `--update` — Update SDF in place to the latest version

  Default value: `false`
* `--uninstall` — Uninstall SDF from the system

  Default value: `false`
* `--quiet` — Do not print sdf console messages

  Default value: `false`
* `--log <LOG>` — Set log level (by setting env var RUST_LOG)
* `--perf <PERF>` — Performance Metric Log Level

  Default value: `0`
* `--dedup` — Deduplicate compiler error messages. This has a slight memory overhead

  Default value: `false`
* `--summary <SUMMARY>` — Save summary of analyzed source files to this file
* `--num-threads <NUM_THREADS>` — Number of threads to use for parallel operations. If unset, default is number of CPU cores. Must be greater than 0 if set
* `--stack-size <STACK_SIZE>` — Set stack size for worker thread in bytes
* `--zero-stmt-sources <ZERO_STMT_SOURCES>` — Set the skip list file
* `--runlog <RUNLOG>` — Save the runlog to this file



## `sdf auth`

Manage credentials and tokens

**Usage:** `sdf auth [OPTIONS] <COMMAND>`

###### **Subcommands:**

* `login` — Obtain credentials for accessing remote resources
* `logout` — Log out of SDF Service
* `status` — Show status of credentials / tokens
* `aws` — Configure how to authenticate with AWS

###### **Options:**

* `--quiet` — Do not print sdf console messages

  Default value: `false`
* `--log <LOG>` — Set log level (by setting env var RUST_LOG)
* `--perf <PERF>` — Performance Metric Log Level

  Default value: `0`
* `--dedup` — Deduplicate compiler error messages. This has a slight memory overhead

  Default value: `false`
* `--summary <SUMMARY>` — Save summary of analyzed source files to this file
* `--num-threads <NUM_THREADS>` — Number of threads to use for parallel operations. If unset, default is number of CPU cores. Must be greater than 0 if set
* `--stack-size <STACK_SIZE>` — Set stack size for worker thread in bytes
* `--zero-stmt-sources <ZERO_STMT_SOURCES>` — Set the skip list file
* `--runlog <RUNLOG>` — Save the runlog to this file



## `sdf auth login`

Obtain credentials for accessing remote resources

Default behavior is to open a browser window and prompt the user to log in.

**Usage:** `sdf auth login [OPTIONS]`

###### **Options:**

* `--id-token <ID_TOKEN>` — Path to a file containing an OIDC identity token (a JWT)
* `--access-key <ACCESS_KEY>` — Access key for headless authentication
* `--secret-key <SECRET_KEY>` — Secret key for headless authentication
* `--credentials-dir <CREDENTIALS_DIR>` — Path to the file where credentials will be stored (default is platform specific)



## `sdf auth logout`

Log out of SDF Service

**Usage:** `sdf auth logout [OPTIONS]`

###### **Options:**

* `--aws` — Delete SDF's AWS Credential State

  Default value: `false`
* `--credentials-dir <CREDENTIALS_DIR>` — Path to the file where credentials are be stored (default is platform specific)



## `sdf auth status`

Show status of credentials / tokens

**Usage:** `sdf auth status [OPTIONS]`

###### **Options:**

* `--credentials-dir <CREDENTIALS_DIR>` — Path to the file where credentials will be stored (default is platform specific)



## `sdf auth aws`

Configure how to authenticate with AWS

**Usage:** `sdf auth aws [OPTIONS]`

###### **Options:**

* `--default-region <DEFAULT_REGION>` — AWS Region to use (default: us-east-1)

  Default value: `us-east-1`
* `--profile <PROFILE>` — AWS profile to use, as usually defined in ~/.aws/config or ~/.aws/credentials
* `--role-arn <ROLE_ARN>` — ARN of the role to assume
* `--external-id <EXTERNAL_ID>` — External ID to use when assuming the role
* `--use-web-identity` — Use web identity to authenticate

  Default value: `false`
* `--bucket-region-map <BUCKET_REGION_MAP>` — Mapping of bucket names to regions
* `--credentials-dir <CREDENTIALS_DIR>` — Path to the file where credentials will be stored (default is platform specific)



## `sdf analyze`

Display results of various supported analyses (e.g. unused-columns)

**Usage:** `sdf analyze [OPTIONS] <COMMAND>`

###### **Subcommands:**

* `unused-columns` — Analyze upstream tables for unused columns

###### **Options:**

* `--quiet` — Do not print sdf console messages

  Default value: `false`
* `--log <LOG>` — Set log level (by setting env var RUST_LOG)
* `--perf <PERF>` — Performance Metric Log Level

  Default value: `0`
* `--dedup` — Deduplicate compiler error messages. This has a slight memory overhead

  Default value: `false`
* `--summary <SUMMARY>` — Save summary of analyzed source files to this file
* `--num-threads <NUM_THREADS>` — Number of threads to use for parallel operations. If unset, default is number of CPU cores. Must be greater than 0 if set
* `--stack-size <STACK_SIZE>` — Set stack size for worker thread in bytes
* `--zero-stmt-sources <ZERO_STMT_SOURCES>` — Set the skip list file
* `--runlog <RUNLOG>` — Save the runlog to this file
* `--cache-dir <CACHE_DIR>` — Sets the 'cache' directory which will contain all output files. Normally, this is `.sdfcache` under the workspace root
* `--credentials-dir <CREDENTIALS_DIR>`



## `sdf analyze unused-columns`

Analyze upstream tables for unused columns

**Usage:** `sdf analyze unused-columns [OPTIONS] [INPUT_FILES]...`

###### **Arguments:**

* `<INPUT_FILES>` — The specification of the output tables used as roots for computing unused upstream columns

###### **Options:**

* `--quiet` — Do not print sdf console messages

  Default value: `false`
* `--log <LOG>` — Set log level (by setting env var RUST_LOG)
* `--perf <PERF>` — Performance Metric Log Level

  Default value: `0`
* `--dedup` — Deduplicate compiler error messages. This has a slight memory overhead

  Default value: `false`
* `--summary <SUMMARY>` — Save summary of analyzed source files to this file
* `--num-threads <NUM_THREADS>` — Number of threads to use for parallel operations. If unset, default is number of CPU cores. Must be greater than 0 if set
* `--stack-size <STACK_SIZE>` — Set stack size for worker thread in bytes
* `--zero-stmt-sources <ZERO_STMT_SOURCES>` — Set the skip list file
* `--runlog <RUNLOG>` — Save the runlog to this file
* `-p`, `--profile <PROFILE>` — Analyze queries using this profile
* `-t`, `--test-profile <TEST_PROFILE>` — Build data using the given --profile but renamed to this



## `sdf show`

Generation of reference documentation: for CLI, metadata definition blocks, information schema and functions

**Usage:** `sdf show [OPTIONS]`

###### **Options:**

* `--functions-yml` — Show available built-in user-defined functions in yml format (default option)
* `--functions-markdown` — Show available built-in user-defined functions in markdown format
* `--cli-markdown` — Show cli commands and options in markdown format
* `--information-schema-ddl` — Show information schema as SQL DDL
* `--information-schema-yml` — Show information schema as yml
* `--information-schema-markdown` — Show information schema in Markdown format
* `--sdf-json-schema` — Show json schema for *.sdf.yml metadata documents
* `--deploy-json-schema` — Show json schema for deploy.json documents
* `--dialect <DIALECT>` — The dialect to show (only for --functions, `` shows all dialects)

  Possible values:
  - `sdf`
  - `spark-sql`:
    SparkSQL Dialect
  - `snowflake`:
    Snowflake Dialect
  - `presto`
  - `bigquery`
  - `redshift`
  - `spark-lp`

* `--section <SECTION>` — The section to show (only for --functions, `` shows all sections)
* `--quiet` — Do not print sdf console messages

  Default value: `false`
* `--log <LOG>` — Set log level (by setting env var RUST_LOG)
* `--perf <PERF>` — Performance Metric Log Level

  Default value: `0`
* `--dedup` — Deduplicate compiler error messages. This has a slight memory overhead

  Default value: `false`
* `--summary <SUMMARY>` — Save summary of analyzed source files to this file
* `--num-threads <NUM_THREADS>` — Number of threads to use for parallel operations. If unset, default is number of CPU cores. Must be greater than 0 if set
* `--stack-size <STACK_SIZE>` — Set stack size for worker thread in bytes
* `--zero-stmt-sources <ZERO_STMT_SOURCES>` — Set the skip list file
* `--runlog <RUNLOG>` — Save the runlog to this file



## `sdf test`

Generation of reference documentation: for CLI, metadata definition blocks, information schema and functions

**Usage:** `sdf test [OPTIONS] [INPUT_FILES]...`

###### **Arguments:**

* `<INPUT_FILES>` — Build these *.sql files or [[catalog.]schema.]table targets; providing no input or a path to workspace.sdf.yml rebuilds the whole workspace

###### **Options:**

* `--profile <PROFILE>` — Build data using this profile
* `--date <DATE>` — Run all tasks that run on this datetime, [ENV] use prefixes of RFC format, e.g. 2020-12-31T21:07, or 2020-12-31, etc
* `--from <FROM>` — Backfill from this datetime (inclusive), use prefixes of RFC format, e.g. 2020-12-31T21:07, or 2020-12-31, etc
* `--to <TO>` — Backfill to this datetime (exclusive), use prefixes of RFC format, e.g. 2020-12-31T21:07, or 2020-12-31, etc (default is now)
* `--dry-run` — Plan but don't execute backfills, output as a shell script in .sdfcache/{profile}/commands.sh

  Default value: `false`
* `--var <VAR>` — Provide var bindings of the form: n=1 or year="2022 01 01", only string variables supported
* `--file-data-format <FILE_DATA_FORMAT>` — Save tables in this format

  Default value: `parquet`

  Possible values: `parquet`, `csv`, `json`

* `--print-data-format <PRINT_DATA_FORMAT>` — Show tables in this format

  Default value: `table`

  Possible values: `table`, `csv`, `tsv`, `json`, `nd-json`

* `--no-cache` — Don't read and write cached table data

  Default value: `false`
* `--cache-use <CACHE_USE>` — Controls read and write cached table data

  Default value: `read-write`

  Possible values: `none`, `read-only`, `read-write`, `write-only`

* `--no-workflow` — Don't run a workflow, just the query

  Default value: `false`
* `--limit <LIMIT>` — "Limiting number of shown rows. Run with --limit 0 to remove limit.",

  Default value: `10`
* `--job-time-limit-s <JOB_TIME_LIMIT_S>` — Impose a hard time limit on the execution of each query. Default is 30 seconds

  Default value: `30`
* `--no-show` — Don't show the result of queries

  Default value: `false`
* `--print-meta-format <PRINT_META_FORMAT>` — Show tables in this format

  Default value: `table`

  Possible values: `table`, `yml`, `json`

* `--generate-information-schema` — Generate information schemas and metadata

  Default value: `false`
* `--no-propagate` — Propagate labels

  Default value: `false`
* `--output-file-contents-on-error` — Include file contents when generating error output

  Default value: `false`
* `--save-assembly` — Save assembly in yaml format

  Default value: `false`
* `--no-parallel` — Do NOT run in parallel
* `--infer-functions` — When queries make use of undeclared functions, generate implied signatures based on how the function is called instead of failing

  Default value: `false`
* `--upload` — Uploads tables to workspace's remote-location (if need to), a user specified s3://bucket

  Default value: `false`
* `--download` — Downloads tables to the workspace's cache (if need to), a user specified s3://bucket

  Default value: `false`
* `--quiet` — Do not print sdf console messages

  Default value: `false`
* `--log <LOG>` — Set log level (by setting env var RUST_LOG)
* `--perf <PERF>` — Performance Metric Log Level

  Default value: `0`
* `--dedup` — Deduplicate compiler error messages. This has a slight memory overhead

  Default value: `false`
* `--summary <SUMMARY>` — Save summary of analyzed source files to this file
* `--num-threads <NUM_THREADS>` — Number of threads to use for parallel operations. If unset, default is number of CPU cores. Must be greater than 0 if set
* `--stack-size <STACK_SIZE>` — Set stack size for worker thread in bytes
* `--zero-stmt-sources <ZERO_STMT_SOURCES>` — Set the skip list file
* `--runlog <RUNLOG>` — Save the runlog to this file
* `--cache-dir <CACHE_DIR>` — Sets the 'cache' directory which will contain all output files. Normally, this is `.sdfcache` under the workspace root
* `--credentials-dir <CREDENTIALS_DIR>`
* `--skip-describe` — Describe data using this profile

  Default value: `false`



## `sdf query`

Generation of reference documentation: for CLI, metadata definition blocks, information schema and functions

**Usage:** `sdf query [OPTIONS] <QUERY> [INPUT_FILES]...`

###### **Arguments:**

* `<QUERY>` — Pass a query before you pass all other arguments
* `<INPUT_FILES>` — Build these *.sql files or [[catalog.]schema.]table targets; providing no input or a path to workspace.sdf.yml rebuilds the whole workspace

###### **Options:**

* `--profile <PROFILE>` — Build data using this profile
* `--date <DATE>` — Run all tasks that run on this datetime, [ENV] use prefixes of RFC format, e.g. 2020-12-31T21:07, or 2020-12-31, etc
* `--from <FROM>` — Backfill from this datetime (inclusive), use prefixes of RFC format, e.g. 2020-12-31T21:07, or 2020-12-31, etc
* `--to <TO>` — Backfill to this datetime (exclusive), use prefixes of RFC format, e.g. 2020-12-31T21:07, or 2020-12-31, etc (default is now)
* `--dry-run` — Plan but don't execute backfills, output as a shell script in .sdfcache/{profile}/commands.sh

  Default value: `false`
* `--var <VAR>` — Provide var bindings of the form: n=1 or year="2022 01 01", only string variables supported
* `--file-data-format <FILE_DATA_FORMAT>` — Save tables in this format

  Default value: `parquet`

  Possible values: `parquet`, `csv`, `json`

* `--print-data-format <PRINT_DATA_FORMAT>` — Show tables in this format

  Default value: `table`

  Possible values: `table`, `csv`, `tsv`, `json`, `nd-json`

* `--no-cache` — Don't read and write cached table data

  Default value: `false`
* `--cache-use <CACHE_USE>` — Controls read and write cached table data

  Default value: `read-write`

  Possible values: `none`, `read-only`, `read-write`, `write-only`

* `--no-workflow` — Don't run a workflow, just the query

  Default value: `false`
* `--limit <LIMIT>` — "Limiting number of shown rows. Run with --limit 0 to remove limit.",

  Default value: `10`
* `--job-time-limit-s <JOB_TIME_LIMIT_S>` — Impose a hard time limit on the execution of each query. Default is 30 seconds

  Default value: `30`
* `--no-show` — Don't show the result of queries

  Default value: `false`
* `--print-meta-format <PRINT_META_FORMAT>` — Show tables in this format

  Default value: `table`

  Possible values: `table`, `yml`, `json`

* `--generate-information-schema` — Generate information schemas and metadata

  Default value: `false`
* `--no-propagate` — Propagate labels

  Default value: `false`
* `--output-file-contents-on-error` — Include file contents when generating error output

  Default value: `false`
* `--save-assembly` — Save assembly in yaml format

  Default value: `false`
* `--no-parallel` — Do NOT run in parallel
* `--infer-functions` — When queries make use of undeclared functions, generate implied signatures based on how the function is called instead of failing

  Default value: `false`
* `--upload` — Uploads tables to workspace's remote-location (if need to), a user specified s3://bucket

  Default value: `false`
* `--download` — Downloads tables to the workspace's cache (if need to), a user specified s3://bucket

  Default value: `false`
* `--quiet` — Do not print sdf console messages

  Default value: `false`
* `--log <LOG>` — Set log level (by setting env var RUST_LOG)
* `--perf <PERF>` — Performance Metric Log Level

  Default value: `0`
* `--dedup` — Deduplicate compiler error messages. This has a slight memory overhead

  Default value: `false`
* `--summary <SUMMARY>` — Save summary of analyzed source files to this file
* `--num-threads <NUM_THREADS>` — Number of threads to use for parallel operations. If unset, default is number of CPU cores. Must be greater than 0 if set
* `--stack-size <STACK_SIZE>` — Set stack size for worker thread in bytes
* `--zero-stmt-sources <ZERO_STMT_SOURCES>` — Set the skip list file
* `--runlog <RUNLOG>` — Save the runlog to this file
* `--cache-dir <CACHE_DIR>` — Sets the 'cache' directory which will contain all output files. Normally, this is `.sdfcache` under the workspace root
* `--credentials-dir <CREDENTIALS_DIR>`



## `sdf obfuscate`

Obfuscate the workspace into the provided directory

**Usage:** `sdf obfuscate [OPTIONS] [INPUT_FILES]...`

###### **Arguments:**

* `<INPUT_FILES>` — Build these *.sql files or [[catalog.]schema.]table targets; providing no input or a path to workspace.sdf.yml rebuilds the whole workspace

###### **Options:**

* `--profile <PROFILE>` — Build data using this profile
* `--date <DATE>` — Run all tasks that run on this datetime, [ENV] use prefixes of RFC format, e.g. 2020-12-31T21:07, or 2020-12-31, etc
* `--from <FROM>` — Backfill from this datetime (inclusive), use prefixes of RFC format, e.g. 2020-12-31T21:07, or 2020-12-31, etc
* `--to <TO>` — Backfill to this datetime (exclusive), use prefixes of RFC format, e.g. 2020-12-31T21:07, or 2020-12-31, etc (default is now)
* `--dry-run` — Plan but don't execute backfills, output as a shell script in .sdfcache/{profile}/commands.sh

  Default value: `false`
* `--var <VAR>` — Provide var bindings of the form: n=1 or year="2022 01 01", only string variables supported
* `--file-data-format <FILE_DATA_FORMAT>` — Save tables in this format

  Default value: `parquet`

  Possible values: `parquet`, `csv`, `json`

* `--print-data-format <PRINT_DATA_FORMAT>` — Show tables in this format

  Default value: `table`

  Possible values: `table`, `csv`, `tsv`, `json`, `nd-json`

* `--no-cache` — Don't read and write cached table data

  Default value: `false`
* `--cache-use <CACHE_USE>` — Controls read and write cached table data

  Default value: `read-write`

  Possible values: `none`, `read-only`, `read-write`, `write-only`

* `--no-workflow` — Don't run a workflow, just the query

  Default value: `false`
* `--limit <LIMIT>` — "Limiting number of shown rows. Run with --limit 0 to remove limit.",

  Default value: `10`
* `--job-time-limit-s <JOB_TIME_LIMIT_S>` — Impose a hard time limit on the execution of each query. Default is 30 seconds

  Default value: `30`
* `--no-show` — Don't show the result of queries

  Default value: `false`
* `--print-meta-format <PRINT_META_FORMAT>` — Show tables in this format

  Default value: `table`

  Possible values: `table`, `yml`, `json`

* `--generate-information-schema` — Generate information schemas and metadata

  Default value: `false`
* `--no-propagate` — Propagate labels

  Default value: `false`
* `--output-file-contents-on-error` — Include file contents when generating error output

  Default value: `false`
* `--save-assembly` — Save assembly in yaml format

  Default value: `false`
* `--no-parallel` — Do NOT run in parallel
* `--infer-functions` — When queries make use of undeclared functions, generate implied signatures based on how the function is called instead of failing

  Default value: `false`
* `--upload` — Uploads tables to workspace's remote-location (if need to), a user specified s3://bucket

  Default value: `false`
* `--download` — Downloads tables to the workspace's cache (if need to), a user specified s3://bucket

  Default value: `false`
* `--quiet` — Do not print sdf console messages

  Default value: `false`
* `--log <LOG>` — Set log level (by setting env var RUST_LOG)
* `--perf <PERF>` — Performance Metric Log Level

  Default value: `0`
* `--dedup` — Deduplicate compiler error messages. This has a slight memory overhead

  Default value: `false`
* `--summary <SUMMARY>` — Save summary of analyzed source files to this file
* `--num-threads <NUM_THREADS>` — Number of threads to use for parallel operations. If unset, default is number of CPU cores. Must be greater than 0 if set
* `--stack-size <STACK_SIZE>` — Set stack size for worker thread in bytes
* `--zero-stmt-sources <ZERO_STMT_SOURCES>` — Set the skip list file
* `--runlog <RUNLOG>` — Save the runlog to this file
* `--cache-dir <CACHE_DIR>` — Sets the 'cache' directory which will contain all output files. Normally, this is `.sdfcache` under the workspace root
* `--credentials-dir <CREDENTIALS_DIR>`
* `--obfuscate-dir <OBFUSCATE_DIR>` — Obfuscate the workspace into the provided directory



   Finished show in 0.006 secs


