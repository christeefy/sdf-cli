---
title: "Large Workspaces"
description:
  "Does your data warehouse have thousands of tables? Can you make a cup of coffee waiting for DBT to compile? If so, this guide is for you!"
---

## Overview

This guide will take you through the process of onboarding a large data warehouse onto SDF. SDF's philosophy - like any good compiler - is completeness, and correctness.

## Prerequisites
 - SDF installed
 - A folder of all DDL files in one environment. Please have the DDLs structured in a folder like `ddls/<catalog>/<schema>/<table>.sql`
 - Your compiled DBT output in the /target folder (see [dbt guide](/integrations/dbt)). Or, your SQL transformations as files.


## Guide

<Steps>
    <Step title="Successfully Load All DDLs">
		Before referencing any transformations, we start by onboarding all DDLs.

        To include your ddls, use the following syntax in your `workspace.sdf.yml`. The *index* directive allows SDF to use the folder structure to reference fully qualified names in a more efficient manner.
		``` 
		includes:
            - path: ddls
            index: catalog-schema-table-name
		```

        To validate that all DDLs are successfully parsed, type bound, and ultimately ingested by SDF, run the following series of commands.
        First - make sure that all DDLs are synctactically valid.
        ``` shell
        sdf compile --stage parse
        ```
        Second - make sure that all names and types are properly resolved.
        ``` shell
        sdf compile --stage resolve
        ```
        Ultimately to run both stages of the compiler:
        ``` shell
        sdf compile
        ```

        From here on, the default will always be `sdf compile`.
        <Note>
        For any DDLs where there are errors, please comment out the offending error or add that file's path to the excludes list. If it is a valid SQL DDL, please report it to the SDF team.
		    </Note>
        <Note>
        In this example, the names of the DDLs follow the directory structure. The directory is structured as `catalog/schema/table` and the sdf-compiled fully qualified names will follow the directory structure. So, a ddl in the path database/analytics/dau_fast.sql will be interpreted as `database.analytics.dau_fast`. You may overwrite this by setting the `catalog` within `defaults` of your workspace globally, or for each includes path individually.
		    </Note>
    </Step>
	<Step title="Add DBT Seed Tables (optional)">
		Skip this part if you do not use DBT Seeds.


        For [DBT seeds](https://docs.getdbt.com/docs/build/seeds). For the 'raw_orders' seed table in the DBT Jaffle Shop example, we would update our `workspace.sdf.yml` as follows:

		```yaml
        ---
		table:
			name: <catalog>.<schema>.<table>
			location: <location>/<to>/<table>.csv
			file-format: csv
			with-header: true
        ```

        Run `sdf compile` to confirm that the seeds tables are properly represented in your schema.
        <Note>
        For Seeds, it's best to add their fully qualified name, including catalog and schema.
		</Note>
    </Step>
    <Step title="Add a Single Compiled Query">
        Now that you are able to run `sdf compile` on all ddls in your warehouse, it's time to add your transformations.

        We start by adding a single SQL transformation. Your *includes* yml will look something like this:
     	``` 
		includes:
            - path: ddls
            index: catalog-schema-table-name
            - path: <queries>/<target/<path1/<file>.sql 
              defaults:
                schema: <default_schema_for_this_query>
                catalog: <default_catalog_for_this_query>
		```   

        To verify your query has been loaded successfully, run `sdf compile` and to see lineage, run `sdf lineage catalog.schema.table`

        `sdf compile` should produce no errors.

    </Step>
    <Step title="Add a Full Directory">
        Assuming that all transformations in one directory are in the same catalog, you may add a whole directory as in the yml below:
                ``` 
		includes:
            - path: ddls
              index: catalog-schema-table-name
            - path: <queries>/<target/<path1>
              index: schema-table-name
              defaults:
                catalog: <default_catalog_for_this_directory>
        #excludes:
        #    - path: <Some file paths to exclude> 
		```
    <Note> 
    In this example, the catalog is given explicitly for the directory we are including, but the schema and the table name will be inferred from the directory tree structure. 
    </Note>
    </Step>
    <Step title="Repeat for All Directories">
        Continue adding transformation directories to your includes list, validating that each addition was successful with `sdf compile` producing no errors.
        ``` 
		includes:
            - path: ddls
              index: catalog-schema-table-name
            - path: <queries>/<target/<path1>
              defaults:
                catalog: <default_catalog_for_this_directory>
            - path: <queries>/<target/<path2>
              defaults:
                catalog: <default_catalog_for_this_directory>
            - path: <queries>/<target/<path3>
              defaults:
                catalog: <default_catalog_for_this_directory>
        #excludes:
        #    - path: <Some file paths to exclude> 
		```
    </Step>
</Steps>

### Excluding Errors
For files that produce errors, please add them to your excludes list. In the same way that SDF can include either a specific file or directory, it can exclude a specific file or directory. Excludes overwrites includes.


To maintain as strong of a dependency tree as possible, it is preferable to only add individual erroring files to excludes rather than whole directories.
``` 
includes:
    - path: <some>/<include>/[path]
    - path: <some>/<include>/file.sql
    - path: <some>/<include>/file.sdf.yml
excludes:
    - path: <some>/<exclude>/[path]
    - path: <some>/<exclude>/file.sql
    - path: <some>/<exclude>/file.sdf.yml
```

<Note>
For any valid file producing an SDF error, please contact the SDF team to report a bug.
</Note>


### Next Steps

Once you have successfully built up your workspace you can move on to defining [Classifers](/guide/classifiers)!
